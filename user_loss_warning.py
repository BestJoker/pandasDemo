# coding:utf-8import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport osimport seaborn as snsimport pandas_profilingfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.svm import SVCimport model_test_indexfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import cross_val_scoreimport joblibpd.options.mode.chained_assignment = None # 默认是'warn'plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签字体。Microsoft YaHei 或 SimHeiplt.rcParams['axes.unicode_minus']=False #用来正常显示负号#获取文件地址def get_file_path(file_name):    PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    path = os.path.join(PROJECT_ROOT, 'data/user_loss_warning/'+file_name)    return path#获取原始数据,return 处理后的x和ydef get_handle_data():    paths = [get_file_path('2019-07-01用户流失预警数据.xlsx'),             get_file_path('2019-08-01用户流失预警数据.xlsx'),             get_file_path('2019-09-01用户流失预警数据.xlsx')]    # paths = [get_file_path('2019-07-01用户流失预警数据.xlsx')]    total_df = pd.DataFrame()    for path in paths:        orign_df = pd.read_excel(path)        total_df = total_df.append(orign_df)    print (total_df.shape)    x,y = handle_data(total_df)    return x,y#处理数据def handle_data(orign_df):    #1.处理count异常值    df = orign_df.copy()    print (df.info())    #数据标准化    x = df.loc[:,df.columns != 'is_silence']    y = df['is_silence']    scaler = StandardScaler() #实例化    x_std = scaler.fit_transform(x) #标准化    print (x.shape,y.shape)    print (y.value_counts())    print (df['is_silence'].value_counts())    return x_std,ydef LR_m(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LR(penalty='l2',solver='saga', C=0.3, max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    # 模型准确率: 72.44 %    # 模型精确率: 6.26 %    # 模型召回率: 85.31 %    # 模型F1值: 11.66 %    # AUC值为： 86.81 %    # KS = 0.59def base_LR(x,y):    print ('base line')    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LR(penalty='l1', solver='saga', C=0.05, max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    print ('-'*30)    # 模型准确率: 72.41 %    # 模型精确率: 6.28 %    # 模型召回率: 85.78 %    # 模型F1值: 11.71 %    # AUC值为： 86.79%    # KS = 0.59def base_RFC(x,y):    rfc = RandomForestClassifier(n_estimators=50,criterion='gini',max_depth=9,random_state=420,class_weight="balanced",n_jobs=-1)    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,x,y,cv=5)    joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc    # 模型准确率: 85.28 %    # 模型精确率: 8.91 %    # 模型召回率: 63.98 %    # 模型F1值: 15.64 %    # AUC值为： 85.79 %    # KS = 0.57def iteration_RFC(x,y):    param_grid = {        'criterion':['gini', 'entropy'],        'max_depth' : np.arange(1, 20, 1)    }    rfc = RandomForestClassifier(random_state=420,class_weight="balanced",n_jobs=-1)    GS = GridSearchCV(rfc, param_grid, cv=10)    GS.fit(x, y)    print (GS.best_params_)    print (GS.best_score_)def base_KNN(x,y):    from sklearn.neighbors import KNeighborsClassifier as KNN    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    knn = KNN(weights="uniform")    knn = knn.fit(X_train,Y_train)    y_predict = knn.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = knn.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(knn,x,y,cv=5)    #0.9755951983643307    print ('-' * 30)    param_grid = {        'weights': ['uniform'],        'n_neighbors': range(1, 20)    }    knn = KNN()    GS = GridSearchCV(knn, param_grid, cv=5)    GS.fit(x, y)    # score = []    # for i in range(1, 15):    #     print (i)    #     knn = KNN(n_neighbors=8,p=i,class_weight="balanced")    #     score_pre = cross_val_score(knn, x, y, cv=5).mean()    #     score.append(score_pre)    # print (range(1, 15)[score.index(max(score))])    # print (max(score))def base_GBDT(x,y):    from sklearn.ensemble import GradientBoostingClassifier as GBDT    from sklearn.model_selection import GridSearchCV    gbdt = GBDT(learning_rate=0.1,                min_samples_split=300,                min_samples_leaf=20,                max_depth=8,                max_features='sqrt',                subsample=0.8,                random_state=10)    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=10)    gbdt = gbdt.fit(X_train,Y_train)    y_predict = gbdt.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = gbdt.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(gbdt,x,y,cv=5)    param_test1 = {'n_estimators': range(20, 81, 10)}    search1 = GridSearchCV(gbdt,param_grid=param_test1,scoring='roc_auc',iid=False,cv=5)    search1.fit(x,y)    print (search1.best_params_)    print (search1.best_scores_)def load_cus_model():    print (1)    try:        model = joblib.load(get_file_path('rfc.pkl'))    except IOError:        x, y = get_handle_data()        model = base_RFC(x, y)        return model    else:        model = joblib.load(get_file_path('rfc.pkl'))        return modeldef predict_data(file_name):    path = get_file_path(file_name)    orign_df = pd.read_excel(path)    x,y = handle_data(orign_df)    y_predcit = model.predict(x)    model_test_index.basic_data_confusion(y, y_predcit)    y_predprob = model.predict_proba(x)    model_test_index.auc_roc_curve(y, y_predprob)    print (y_predcit)    print (y_predprob)#SVC调参gammadef rbf_gamma_line(gamma_range, X, y):    score = []    for i in gamma_range:        print (len(score))        Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=420)        clf = SVC(kernel='rbf', gamma=i, cache_size=5000,class_weight='balanced').fit(Xtrain, Ytrain)        score.append(clf.score(Xtest, Ytest))    max_score = max(score)    gamma = gamma_range[score.index(max_score)]    print (' gamma：%f \n max_score：%f' % (gamma, max_score))    fig = plt.figure(figsize=(10, 8))    plt.plot(gamma_range, score)    plt.show()    #gamma=0def rbf_C_line(C_range,X,y):    score = []    for i in C_range:        print (len(score))        X = StandardScaler().fit_transform(X)        Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420)        clf = SVC(kernel='rbf',C=i,cache_size=5000,class_weight='balanced').fit(Xtrain,Ytrain)        score.append(clf.score(Xtest,Ytest))    max_score = max(score)    C = C_range[score.index(max_score)]    print (' C：%f \n max_score：%f' %(C,max_score))    fig = plt.figure(figsize=(10,8))    plt.plot(C_range,score)    plt.show()    #c=0.01,max_score=0.978674def base_SVC(x,y):    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    clf = SVC(kernel='rbf', C=20, cache_size=5000,class_weight='balanced').fit(X_train, Y_train)    y_predict = clf.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    model_test_index.corss_val_score_cus(clf,x,y,cv=5)    # # 模型准确率: 71.20 %    # 模型精确率: 5.89 %    # 模型召回率: 83.41 %    # 模型F1值: 11.00 %    # gamma_range = np.logspace(-10, 1, 50)    # rbf_gamma_line(gamma_range, x, y)    # C_range = np.linspace(0.01,30,50)    # 继续缩小精确范围    # C_range = np.linspace(0.01, 2, 30)    # rbf_C_line(C_range, x, y)#初步筛选模型 调用def test_models(x,y):    from sklearn.neighbors import KNeighborsClassifier    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis    from sklearn.naive_bayes import GaussianNB    from sklearn.svm import SVC    from sklearn.linear_model import LogisticRegression    from sklearn.tree import DecisionTreeClassifier    from sklearn.model_selection import KFold    from sklearn.ensemble import RandomForestClassifier    # prepare models    models = []    models.append(('LR', LogisticRegression()))    models.append(('LDA', LinearDiscriminantAnalysis()))    models.append(('KNN', KNeighborsClassifier()))    models.append(('CART', DecisionTreeClassifier()))    models.append(('NB', GaussianNB()))    models.append(('SVM', SVC()))    models.append(('RFC',RandomForestClassifier()))    # evaluate each model in turn    results = []    names = []    scoring = 'accuracy'    for name, model in models:        kfold = KFold(n_splits=10, random_state=7)        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)        results.append(cv_results)        names.append(name)        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())        print(msg)    # boxplot algorithm comparison    fig = plt.figure()    fig.suptitle('Algorithm Comparison')    ax = fig.add_subplot(111)    plt.boxplot(results)    ax.set_xticklabels(names)    plt.show()#主函数if __name__ == '__main__':    model = load_cus_model()    x,y = get_handle_data()    base_SVC(x,y)    # test_models(x,y)    # predict_data('2019-10-01用户流失预警数据.xlsx')