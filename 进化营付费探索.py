# coding:utf-8import pandas as pdimport numpy as npimport osimport matplotlib.pyplot as pltimport scipy.stats as stimport mathimport seaborn as snsfrom time import timeimport datetimefrom sklearn.preprocessing import OneHotEncoderimport model_test_indexfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_scorefrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.naive_bayes import GaussianNBfrom sklearn.svm import SVCfrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import KFoldfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.utils.class_weight import compute_sample_weightpd.options.mode.chained_assignment = None # 默认是'warn'plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签字体。Microsoft YaHei 或 SimHeiplt.rcParams['axes.unicode_minus']=False #用来正常显示负号#初步探索选择模型def data_handel(orign_df):    columns = [        '注册至今时间(天)',        '是否新注册用户',        '是否入群',        '是否激活',        '开营前7天活跃天数',        '营期间活跃天数',        '先导课听课时长(分钟)',        '看直播课程数',        #     '看直播总时长(分钟)',        #     '看直播回放总时长(分钟)',        '看直播总互动数',        '非转化营直播课看课总时长(分钟)',        '全部课程分享次数',        '课表访问次数',        '支付页面浏览次数',        '是否购买大学籍'    ]    temp_df = orign_df[columns]    # 1.将分布不均匀数据进行取对数    # log_columns = [    #     '注册至今时间(天)',    #     '先导课听课时长(分钟)',    #     '先导课听课时长(分钟)',    #     '非转化营直播课看课总时长(分钟)',    #     '看直播总时长(分钟)'    # ]    # for column in log_columns:    #     orign_df[column] = orign_df[column].apply(np.log)    #     print column    # 2.将分类调整为0和1    temp_df['是否新注册用户'] = temp_df['是否新注册用户'].apply(lambda x: 1 if x == '是' else 0)    temp_df['是否入群'] = temp_df['是否入群'].apply(lambda x: 1 if x == '是' else 0)    temp_df['是否激活'] = temp_df['是否激活'].apply(lambda x: 1 if x == '是' else 0)    temp_df['注册至今时间(天)'] = temp_df['注册至今时间(天)'].fillna(value=0)    scaler = StandardScaler()  # 实例化    X = temp_df.iloc[:, temp_df.columns != '是否购买大学籍']    y = temp_df.iloc[:, temp_df.columns == '是否购买大学籍'].values.ravel()    trans_x = scaler.fit_transform(X)    return trans_x,ydef choice_model(x,y):    # prepare models    models = []    models.append(('LR', LogisticRegression()))    models.append(('LDA', LinearDiscriminantAnalysis()))    models.append(('KNN', KNeighborsClassifier()))    models.append(('CART', DecisionTreeClassifier()))    # models.append(('NB', GaussianNB()))    models.append(('SVM', SVC()))    models.append(('RFC',RandomForestClassifier()))    # evaluate each model in turn    results = []    names = []    scoring = 'accuracy'    for name, model in models:        kfold = KFold(n_splits=10, random_state=420)        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)        results.append(cv_results)        names.append(name)        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())        print(msg)    # boxplot algorithm comparison    fig = plt.figure()    fig.suptitle('Algorithm Comparison')    ax = fig.add_subplot(111)    plt.boxplot(results)    ax.set_xticklabels(names)    plt.show()    # LR: 0.986738(0.009050)    # LDA: 0.973845(0.014471) --    # KNN: 0.986615(0.008739)    # CART: 0.981213(0.012598) --    # SVM: 0.985757(0.010966) --    # RFC: 0.987353(0.009087)def base_LR(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.05, max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    print ('-'*30)    # 模型准确率: 93.78 %    # 模型精确率: 18.92 %    # 模型召回率: 94.59 %    # 模型F1值: 31.53 %    # AUC值为： 97.89 %    # KS = 0.9    # L1 = LogisticRegression(penalty='l2', solver='sag', C=0.05, max_iter=1000, class_weight='balanced')    # 模型准确率: 93.82 %    # 模型精确率: 19.02 %    # 模型召回率: 94.59 %    # 模型F1值: 31.67 %    # AUC值为： 97.85 %    # KS = 0.89# {'C': 0.01, 'class_weight': 'balanced', 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}# 0.9373717610527426# 模型准确率: 94.02 %# 模型精确率: 19.89 %# 模型召回率: 97.30 %# 模型F1值: 33.03 %# AUC值为： 97.78 %# KS = 0.92def iteration_LR(x,y):    # param_grid = {    #     'penalty': ['l1'],    #     'solver': ['liblinear','saga'],    #     'C': np.arange(0.01,1,0.1),    #     'class_weight':['balanced'],    #     'max_iter':range(100,1500,100)    # }    #    # lr = LogisticRegression()    # GS = GridSearchCV(lr, param_grid, cv=10)    # GS.fit(x, y)    # print (GS.best_params_) #    # print (GS.best_score_) #    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LogisticRegression(penalty='l1', solver='saga', C=0.01, max_iter=100, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    print (L1.coef_)    print (L1.intercept_)    print ('-'*30)# 模型准确率: 98.73%# 模型精确率: 87.50%# 模型召回率: 18.92%# 模型F1值: 31.11%# AUC值为： 98.43%# KS = 0.89# 基于10折交叉验证的随机森林模型准确率:0.9856def base_RFC(x,y):    rfc = RandomForestClassifier(n_estimators=41,criterion='gini',max_depth=14,random_state=420,n_jobs=-1)    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    sample_weight = [1.7 if i == 1 else 1 for i in Y_train]    sample_weight = np.array(sample_weight)    rfc = rfc.fit(X_train,Y_train,sample_weight=sample_weight)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,x,y,cv=10)    # joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc# 模型准确率: 98.69%# 模型精确率: 77.78%# 模型召回率: 18.92%# 模型F1值: 30.43%# AUC值为： 96.65%# KS = 0.88# 基于10折交叉验证的随机森林模型准确率:0.987def iteration_RFC(x,y):    # score = []    # for i in range(5, 60, 1):    #     rfc = RandomForestClassifier(n_estimators=i + 1, n_jobs=-1, random_state=420)    #     s = cross_val_score(rfc, x, y, cv=10).mean()    #     score.append(s)    # print (5 + score.index(max(score)) * 1 + 1, max(score))    # plt.figure(figsize=(20, 5))    # plt.plot(range(6, 61, 1), score, color='red')    # plt.show()    n_estimators=41    # param_grid = {    #     'criterion': ['gini', 'entropy'],    #     'max_depth': np.arange(1, 20, 1)    # }    # rfc = RandomForestClassifier(n_estimators=41, random_state=420,n_jobs=-1)    # GS = GridSearchCV(rfc, param_grid, cv=10)    # GS.fit(x, y)    #    # print (GS.best_params_) #    # print (GS.best_score_) #    # {'criterion': 'gini', 'max_depth': 14}    # 0.9869813539138692    rfc = RandomForestClassifier(n_estimators=41,criterion='gini',max_depth=14,random_state=420,n_jobs=-1)    X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=420)    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,X,y,cv=10)    # joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc# 模型准确率: 96.28%# 模型精确率: 17.07%# 模型召回率: 37.84%# 模型F1值: 23.53%# 0.9627507163323782# AUC值为： 95.32%# KS = 0.85def iteration_SVM(X,y):    # X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=420)    # score = []    # gamma_range = np.logspace(-10, 1, 50)  # 返回在对数刻度上均匀间隔的数字    # print (gamma_range)    # for i in gamma_range:    #     clf = SVC(kernel='rbf', gamma=i, cache_size=5000).fit(X_train, Y_train)    #     score.append(clf.score(X_test, Y_test))    # print (max(score), gamma_range[score.index(max(score))])    # plt.plot(gamma_range, score)    # plt.show()    # 0.986901350798199    # 0.020235896477251554    # X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=420)    # score = []    # C_range = np.linspace(0.01, 30, 50)    # for i in C_range:    #     clf = SVC(kernel='rbf', C=i, gamma=0.020235896477251554, cache_size=5000).fit(X_train, Y_train)    #     score.append(clf.score(X_test, Y_test))    #    # print (max(score), C_range[score.index(max(score))])    # plt.plot(C_range, score)    # plt.show()    # 0.9881293491608678    # 3.070204081632653    param_grid = {        'kernel': ['rbf'],        'gamma' : np.logspace(-10, 1, 50),        'C': np.linspace(0.01, 30, 50),        'class_weight': ['balanced']    }    lr = SVC()    GS = GridSearchCV(lr, param_grid, cv=5)    GS.fit(X, y)    print (GS.best_params_)  #    print (GS.best_score_)  #    # X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=420)    # clf = SVC(kernel='rbf',gamma=0.020235896477251554,C=3.070204081632653, cache_size=5000,class_weight='balanced',probability=True)    # clf = clf.fit(X_train, Y_train)    # y_predcit = clf.predict(X_test)    # model_test_index.basic_data_confusion(Y_test, y_predcit)    #    # y_predprob = clf.predict_proba(X_test)    # print (clf.score(X_test, Y_test).mean())    #    # model_test_index.auc_roc_curve(Y_test, y_predprob)    # clf = SVC(kernel='rbf',gamma=0.020235896477251554,C=3.070204081632653, cache_size=5000,class_weight='balanced',probability=True)    print ('-'*30)#主函数if __name__ == '__main__':    PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    path = os.path.join(PROJECT_ROOT,'data/进化营用户探索.xlsx')    orign_df = pd.read_excel(path)    str_columns = orign_df.select_dtypes(include=['object', 'datetime']).columns    num_columns = orign_df.select_dtypes(include=['number']).columns    orign_df[str_columns] = orign_df[str_columns].fillna('')    orign_df[num_columns] = orign_df[num_columns].fillna(value=0)    orign_df.info()    X,y = data_handel(orign_df)    # choice_model(X,y)    # base_LR(X,y)    # iteration_LR(X,y)    iteration_SVM(X,y)