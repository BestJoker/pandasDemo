# coding:utf-8import pandas as pdimport numpy as npimport osimport matplotlib.pyplot as pltimport scipy.stats as stimport mathimport seaborn as snsfrom time import timeimport datetimefrom sklearn.preprocessing import OneHotEncoderimport model_test_indexfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_scorefrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.naive_bayes import GaussianNBfrom sklearn.svm import SVCfrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import KFoldfrom sklearn.ensemble import RandomForestClassifierpd.options.mode.chained_assignment = None # 默认是'warn'plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签字体。Microsoft YaHei 或 SimHeiplt.rcParams['axes.unicode_minus']=False #用来正常显示负号#第一轮初步筛选寻找高相关参数def first_choice(orign_df):    columns = [        'live_course_counts',        'is_in_group',        'regist_now_days',        'sum_duration_min',        'is_pay',        'has_watch_live',        'sum_12_7_duration_min',        'sum_12_1_duration_min',        'schdule_counts',        'share_times',        'live_share_times',        'before_ccy_7_daus',        'ccy_dur_daus',        '看直播数量(int)',        '看直播时长(min)',        '看直播课的回放时长(min)',        '非转化营直播课程的看课时长(min)',        '理论课时长(min)',        '前沿课时长(min)',        '理论课看课数量',        '前沿课看课数量'    ]    temp_df = orign_df[orign_df['user_status_type']=='9.9冲刺营']    temp_df = temp_df[columns]    temp_df['is_in_group'] = temp_df['is_in_group'].map({        '不在群里':0,        '在群里':1    })    a = temp_df.corr()    f, ax = plt.subplots(figsize=(20, 20), dpi=180)    # 调色板ma    cmap = sns.diverging_palette(220, 10, as_cmap=True)    sns.heatmap(a, ax=ax, square=True, lw=0.3, cmap=cmap, annot=True)    ax.set_title('相关系数矩阵', fontsize=20)    plt.show()    #筛选出来的有用的指标为：    #schdule_counts    #share_times，live_share_times，二者取其一    #before_ccy_7_daus    #ccy_dur_daus    #看直播数量(int)，看直播时长(min)，二者取其一    #非转化营直播课程的看课时长(min)，前沿课时长(min)，二者取其一    #理论课时长(min)    #is_in_group:可能因为分类问题导致相关性不强，但是我们带上    # 6.关键因素探索：    # 1）直播时长和直播数量：看直播数量>=2次转化率从1次的6.8%->14%，看直播时长>=60min转化率从6.7%->13.0%    # 2）非转化营直播课程时长：非转化营直播课程时长>=60min 转化率从6.6%->15.8%    # 3）开营前7天和营进行中5天活跃情况：开营前活跃>=3天 和 开营中活跃>=2天以上存在明显拐点    # 4）开营期间全部课程分享次数(相关性同营直播分享次数)：全部课程分享次数>=1次的用户转化率从7.9%->17.5%    # 5）开营期间课表访问次数：课表访问次数>=2次转化率从12.9%->20.0%    # 6）注册至今时间：注册至今时间<700天，转化率相对比较高# base:  调参无效# 模型准确率: 97.73 %# 模型精确率: 57.14 %# 模型召回率: 11.76 %# 模型F1值: 19.51 %# AUC值为： 63.41 %def base_KNN(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    knn = KNeighborsClassifier(n_neighbors=5,weights="uniform")    knn = knn.fit(X_train,Y_train)    y_predict = knn.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = knn.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(knn,x,y,cv=5)    print ('-' * 30)    # param_grid = {    #     'weights': ['uniform','distance'],    #     'n_neighbors': range(1, 20)    # }    #    # knn = KNeighborsClassifier()    # GS = GridSearchCV(knn, param_grid, cv=10)    # GS.fit(x, y)    #    # print (GS.best_params_) #{'n_neighbors': 5, 'weights': 'uniform'}    # print (GS.best_score_) #0.9753826587961744# 模型准确率: 81.32 %# 模型精确率: 7.53 %# 模型召回率: 61.76 %# 模型F1值: 13.42 %# AUC值为： 82.35 %def base_LR(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LogisticRegression(penalty='l1', solver='saga', C=0.05, max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    print ('-'*30)# 模型准确率: 81.67 %# 模型精确率: 7.97 %# 模型召回率: 64.71 %# 模型F1值: 14.19 %# AUC值为： 82.31 %def iteration_LR(x,y):    # param_grid = {    #     'penalty': ['l1'],    #     'solver': ['liblinear','saga'],    #     'C': np.arange(0.01,1,0.5),    #     'class_weight':['balanced'],    #     'max_iter':range(300,1000,200)    # }    #    # lr = LogisticRegression()    # GS = GridSearchCV(lr, param_grid, cv=10)    # GS.fit(x, y)    # print (GS.best_params_) #    # print (GS.best_score_) #    #test1：    # {'C': 0.4, 'class_weight': 'balanced', 'max_iter': 300, 'penalty': 'l1', 'solver': 'saga'}    # 0.7804557432027788    # 模型准确率: 80.98 %    # 模型精确率: 7.39 %    # 模型召回率: 61.76 %    # 模型F1值: 13.21 %    # AUC值为： 82.22 %    # KS = 0.55    #test2:    # {'C': 0.01, 'class_weight': 'balanced', 'max_iter': 300, 'penalty': 'l1', 'solver': 'saga'}    # 0.7816975514603973    # 模型准确率: 81.67 %    # 模型精确率: 7.97 %    # 模型召回率: 64.71 %    # 模型F1值: 14.19 %    # AUC值为： 82.31 %    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LogisticRegression(penalty='l1', solver='saga', C=0.01, max_iter=300, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    return L1    print ('-'*30)### SVM# linear# The accuracy under kernel linear is 0.976568# 00:00:034373# poly# The accuracy under kernel poly is 0.976568# 00:00:019715# rbf# The accuracy under kernel rbf is 0.976568# 00:00:092359# sigmoid# The accuracy under kernel sigmoid is 0.965541# 00:00:051880##  best：linear 0.976568def choose_kernel(X, y):    X = StandardScaler().fit_transform(X)    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=420)    Kernel = ['linear', 'poly', 'rbf', 'sigmoid']    score = []    for kernel in Kernel:        print (kernel)        time0 = time()        clf = SVC(kernel=kernel                  , gamma='auto'                  , degree=1                  , cache_size=5000                  ).fit(Xtrain, Ytrain)        score.append(clf.score(Xtest, Ytest))        print ('The accuracy under kernel %s is %f' % (kernel, clf.score(Xtest, Ytest)))        print (datetime.datetime.fromtimestamp(time() - time0).strftime('%M:%S:%f'))        # 获取最大的分数和对应的kernel    index = score.index(max(score))    best_kernel = Kernel[index]    print ('\n best：%s %f' % (best_kernel, max(score)))# gamma：0.268270# max_score：0.977257def rbf_gamma_line(gamma_range, X, y):    score = []    for i in gamma_range:        X = StandardScaler().fit_transform(X)        Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=420)        clf = SVC(kernel='rbf', gamma=i, cache_size=5000).fit(Xtrain, Ytrain)        score.append(clf.score(Xtest, Ytest))    max_score = max(score)    gamma = gamma_range[score.index(max_score)]    print (' gamma：%f \n max_score：%f' % (gamma, max_score))    fig = plt.figure(figsize=(10, 8))    plt.plot(gamma_range, score)    plt.show()# C：0.970690#  max_score：0.977257def rbf_C_line(C_range,X,y):    score = []    for i in C_range:        X = StandardScaler().fit_transform(X)        Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420)        clf = SVC(kernel='rbf',gamma=0.268270,C=i,cache_size=5000).fit(Xtrain,Ytrain)        score.append(clf.score(Xtest,Ytest))    max_score = max(score)    C = C_range[score.index(max_score)]    print (' C：%f \n max_score：%f' %(C,max_score))    fig = plt.figure(figsize=(10,8))    plt.plot(C_range,score)    plt.show()# 模型准确率: 97.73%# 模型精确率: 100.00%# 模型召回率: 2.94%# 模型F1值: 5.71%def iteration_SVM(X,y):    X = StandardScaler().fit_transform(X)    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=420)    clf = SVC(kernel='rbf', gamma=0.268270,C=0.970690, cache_size=5000)    clf = clf.fit(X_train, Y_train)    y_predcit = clf.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    # y_predprob = clf.predict_proba(X_test)    print (clf.score(X_test, Y_test).mean())    # model_test_index.auc_roc_curve(Y_test, y_predprob)    print ('-'*30)# 模型准确率: 97.52%# 模型精确率: 25.00%# 模型召回率: 2.94%# 模型F1值: 5.26%# AUC值为： 75.99%def base_RFC(x,y):    rfc = RandomForestClassifier(n_estimators=51,random_state=420,n_jobs=-1)    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,x,y,cv=5)    # joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc# 模型准确率: 97.59%# 模型精确率: 33.33%# 模型召回率: 2.94%# 模型F1值: 5.41%# AUC值为： 76.54%def iteration_RFC(X,y):    # score = []    # for i in range(40, 60, 1):    #     rfc = RandomForestClassifier(n_estimators=i + 1, n_jobs=-1, random_state=420)    #     s = cross_val_score(rfc, x, y, cv=10).mean()    #     score.append(s)    # print (40 + score.index(max(score)) * 1 + 1, max(score))    # plt.figure(figsize=(20, 5))    # plt.plot(range(41, 61, 1), score, color='red')    # plt.show()    # n_estimators=51    # param_grid = {    #     'criterion': ['gini', 'entropy'],    #     'max_depth': np.arange(1, 20, 1)    # }    # rfc = RandomForestClassifier(n_estimators=51, random_state=420,n_jobs=-1)    # GS = GridSearchCV(rfc, param_grid, cv=10)    # GS.fit(x, y)    #    # print (GS.best_params_) #    # print (GS.best_score_) #    # {'criterion': 'gini', 'max_depth': 17}    # 0.9751766678644149    rfc = RandomForestClassifier(n_estimators=51,criterion='gini',max_depth=17,random_state=420,n_jobs=-1)    X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=420)    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,X,y,cv=5)    # joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc#初步探索选择模型def data_handel(orign_df):    temp_df = orign_df.copy(deep=True)    columns = [        'schdule_counts',        'share_times',        'before_ccy_7_daus',        'ccy_dur_daus',        '看直播时长(min)',        '非转化营直播课程的看课时长(min)',        '理论课时长(min)',        'is_pay',        'is_in_group'    ]    temp_df = temp_df[columns]    #调整is_in_group的分类    season_dummy = pd.get_dummies(temp_df['is_in_group'],prefix='is_in_group')    temp_df = pd.concat([temp_df,pd.DataFrame(season_dummy)],axis=1)    temp_df.drop(['is_in_group'],axis=1,inplace=True)    #数据标准化    X = temp_df[['看直播时长(min)','非转化营直播课程的看课时长(min)','理论课时长(min)']].values    scaler = StandardScaler()  # 实例化    x_std = scaler.fit_transform(X)    temp_df[['看直播时长(min)', '非转化营直播课程的看课时长(min)', '理论课时长(min)']] = x_std    X = temp_df.iloc[:, temp_df.columns != 'is_pay']    y = temp_df['is_pay']    print ('-'*30)    iteration_LR(X,y)    # choice_model(X,y)    # KNN:0.974966 (0.011124)    # LR:0.974138 (0.010404)    # SVM:0.974345 (0.009981)    # RFC:0.973724 (0.010274)def choice_model(x,y):    # prepare models    models = []    models.append(('LR', LogisticRegression()))    models.append(('LDA', LinearDiscriminantAnalysis()))    models.append(('KNN', KNeighborsClassifier()))    models.append(('CART', DecisionTreeClassifier()))    models.append(('NB', GaussianNB()))    models.append(('SVM', SVC()))    models.append(('RFC',RandomForestClassifier()))    # evaluate each model in turn    results = []    names = []    scoring = 'accuracy'    for name, model in models:        kfold = KFold(n_splits=10, random_state=7)        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)        results.append(cv_results)        names.append(name)        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())        print(msg)    # boxplot algorithm comparison    fig = plt.figure()    fig.suptitle('Algorithm Comparison')    ax = fig.add_subplot(111)    plt.boxplot(results)    ax.set_xticklabels(names)    plt.show()    # choice_model(X,y)    # KNN:0.974966 (0.011124)    # LR:0.974138 (0.010404)    # SVM:0.974345 (0.009981)    # RFC:0.973724 (0.010274)#主函数if __name__ == '__main__':    PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    path = os.path.join(PROJECT_ROOT,'data/认知进化营同学明细数据-英文版.xlsx')    orign_df = pd.read_excel(path)    str_columns = orign_df.select_dtypes(include=['object', 'datetime']).columns    num_columns = orign_df.select_dtypes(include=['number']).columns    orign_df[str_columns] = orign_df[str_columns].fillna('')    orign_df[num_columns] = orign_df[num_columns].fillna(value=0)    orign_df.info()    # first_choice(orign_df)    data_handel(orign_df)