# coding:utf-8import pandas as pdimport numpy as npimport osimport matplotlib.pyplot as pltimport seaborn as snsimport datetimeimport randomimport pandas_profilingimport model_test_indexfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorepd.options.mode.chained_assignment = None # 默认是'warn'plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签字体。Microsoft YaHei 或 SimHeiplt.rcParams['axes.unicode_minus']=False #用来正常显示负号'''3. 项目目的：根据客户信息，建立违约模型，利用AUC和KS值作为评价指标，最终选出评价效果最好的预测模型。'''#获取文件地址def get_file_path(file_name):    PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    path = os.path.join(PROJECT_ROOT, 'data/user-default-data/'+file_name)    return path#获取原始数据,return 数据+是否处理后def get_handle_data():    handle_path = get_file_path('user-default-handle-data.csv')    try:        handle_df = pd.read_csv(handle_path)    except IOError:        print('没有处理过的数据，读取原始数据')        path = get_file_path('cs-training.csv')        orign_df = pd.read_csv(path)        watch_data(orign_df)        handle_df = handle_data(orign_df)        return handle_df    else:        print ('数据已经处理过，直接读取')        handle_df = pd.read_csv(handle_path)        return handle_df#观察数据def watch_data(orign_df):    #1.处理count异常值    df = orign_df.copy()    print (df.info())    print (df.describe().T)    ########## 探索中发现的问题 ##########    # 1.数据共150000条记录，检查ID是否重复    print (df.shape[0]-df["Unnamed: 0"].nunique())  #无重复ID    # 2.检查是否有重复的变量名    print (len(df.columns) - df.columns.nunique()) #无重复变量名    # 3.变量DebtRatio 存在异常值    DebtRatio_num = df.loc[df['DebtRatio']>1,'DebtRatio'].shape[0]    DebtRatio_per = df.loc[df['DebtRatio'] > 1,'DebtRatio'].shape[0] / df[        "Unnamed: 0"].nunique()    print('DebtRatio的异常值共 {} 个'.format(DebtRatio_num))    print('DebtRatio的异常值占比 {}% '.format(round(DebtRatio_per, 2) * 100))    # 4.MonthlyIncome 上分位数是8249，最大值是3008750，说明可能存在异常值异常值    # f, ax = plt.subplots(figsize=(10, 5))    # sns.distplot(df['MonthlyIncome'])    # ax.set_title('Distribution of MonthlyIncome')    # plt.show()    # 结论：月收入主要集中在500000以内，还需要统计大于500000的数量    print ((df['MonthlyIncome']>500000).sum())    # 输出：12    # 结果：超过500000只有12个，这几个ID的收入可能是异常值，可用500000代替原有的值(仍然是高收入，并不会影响实际含义)    # 生成报告    # report = pandas_profiling.ProfileReport(df)    # print (report)    # report.to_file("output_file.html")#处理数据def handle_data(orign_df):    #1.处理count异常值    df = orign_df.copy()    #1.剔除age=0的样本    df = df.drop(index=df[df['age'] == 0].index, axis=0)    # 2.将MonthlyIncome中大于500000的值替换为500000    df.loc[df['MonthlyIncome']>500000,'MonthlyIncome'] = 500000    # 3.填补MonthlyIncome的缺失值    # 探究收入和违约率之间的关系    # 收入低于0.25分位数的违约率    bad_rate_25 = (df.loc[df['MonthlyIncome']<df['MonthlyIncome'].describe()['25%'],'SeriousDlqin2yrs']).mean()    bad_rate_75 = (df.loc[df['MonthlyIncome']>df['MonthlyIncome'].describe()['75%'],'SeriousDlqin2yrs']).mean()    bad_rate_25_75 = (df.loc[(df['MonthlyIncome']>=df['MonthlyIncome'].describe()['25%']) & (df['MonthlyIncome']<=df['MonthlyIncome'].describe()['75%']),'SeriousDlqin2yrs']).mean()    print('收入低于0.25分位数的违约率是{}'.format(round(bad_rate_25, 2)))    print('收入位于0.25和0.75分位数之间的违约率是{}'.format(round(bad_rate_25_75, 2)))    print('收入超过0.75分位数的违约率是{}'.format(round(bad_rate_75, 2)))    # 结论：收入越高，违约率越低，收入缺失值的填充对模型有很大影响，需要找到和收入相关的变量，保证填充的合理性    #年龄与收入之间的关系    # plt.figure(figsize=(15,5))    # sns.relplot(x='age',y='MonthlyIncome',data=df)    # plt.ylim([0,100000])    # plt.show()    # 因为收入越低，违约率越高，为了尽可能识别出高违约率客户，使用较低的收入填充    # 年龄段不同，收入不同，用每个年龄段的0.05分位数填充对应年龄段的收入    MonthlyIncome_20_40 = df.loc[df['age']<=40,'MonthlyIncome'].describe(percentiles=[0.05])['5%']    MonthlyIncome_40_60 = df.loc[(df['age']>40) & (df['age']<=60),'MonthlyIncome'].describe(percentiles=[0.05])['5%']    MonthlyIncome_60 = df.loc[df['age']>60,'MonthlyIncome'].describe(percentiles=[0.05])['5%']    df.loc[df['age'] <= 40, 'MonthlyIncome'] = df.loc[df['age'] <= 40, 'MonthlyIncome'].fillna(value=MonthlyIncome_20_40)    df.loc[(df['age'] > 40) & (df['age'] <= 60), 'MonthlyIncome'] = df.loc[(df['age']>40) & (df['age']<=60),'MonthlyIncome'].fillna(value=MonthlyIncome_40_60)    df.loc[df['age'] > 60, 'MonthlyIncome'] = df.loc[df['age']>60,'MonthlyIncome'].fillna(value=MonthlyIncome_60)    # 4.处理DebtRatio 中的异常值    bad_rate_25 = (df.loc[df['DebtRatio']<df['DebtRatio'].describe()['25%'],'SeriousDlqin2yrs']).mean()    bad_rate_75 = (df.loc[df['DebtRatio']>df['DebtRatio'].describe()['75%'],'SeriousDlqin2yrs']).mean()    bad_rate_25_75 = (df.loc[(df['DebtRatio']>=df['DebtRatio'].describe()['25%']) & (df['DebtRatio']<=df['DebtRatio'].describe()['75%']),'SeriousDlqin2yrs']).mean()    print('DebtRatio低于0.25分位数的违约率是{}'.format(round(bad_rate_25, 2)))    print('DebtRatio位于0.25和0.75分位数之间的违约率是{}'.format(round(bad_rate_25_75, 2)))    print('DebtRatio超过0.75分位数的违约率是{}'.format(round(bad_rate_75, 2)))    # 随机抽取DebtRatio中的正常值替换异常值    normal_DebeRatio = df[df['DebtRatio']<=1]['DebtRatio']    df[df['DebtRatio'] > 1]['DebtRatio'] = df[df['DebtRatio'] > 1]['DebtRatio'].map(normal_DebeRatio.sample(1))    # 5.剔除高相关变量，避免多重共线性    df.drop(['NumberOfTime60-89DaysPastDueNotWorse','NumberOfTimes90DaysLate'],inplace=True,axis=1)    # 输出：如下图    # 结论：NumberOfDependents的取值主要集中在[0,1,2,3,4]中，从[0,1,2,3,4]随机抽取值进行填充    # 6.填充NumberOfDependents的缺失值    # sns.countplot(x='NumberOfDependents', data=df)    # plt.show()    # 填充缺失值    Dependents = pd.Series([0,1,2,3,4])    df['NumberOfDependents'].fillna(value=Dependents.sample(1).values[0],inplace=True)    # 处理异常值，根据分布情况可以看出，家庭人口数超过8人的非常少，再结合生活常识，将超过8人的全部用8代替    df.loc[df['NumberOfDependents']>8,'NumberOfDependents'] = 8    #保存处理后的数据    path = get_file_path('user-default-handle-data.csv')    df.to_csv(path,index=False)    print ('保存处理数据后数据成功')    return df#根据处理后数据归一化数据，并返回归一化之后的X和ydef scaler_data(handle_df):    # 8.变量的数值差异太大，建模时可能影响某些变量的重要性，所以对变量进行归一化处理，将变量值变为[0,1]之间    from sklearn.preprocessing import MinMaxScaler    df = handle_df.copy()    X = df.iloc[:, ~df.columns.isin(['Unnamed: 0', 'SeriousDlqin2yrs'])]    y = df['SeriousDlqin2yrs']    scaler = MinMaxScaler()    # 归一化数据    X_scale = scaler.fit_transform(X)    # 9.SeriousDlqin2yrs 的分布说明样本好坏比严重失衡，在建模时利用class_weight='balanced'解决    print (df.SeriousDlqin2yrs.value_counts())    return X,y#LogisticRegressiondef logisticRegression_m(X,y):    from sklearn.linear_model import LogisticRegression as LR    X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)    # l1 = []    # l2 = []    # l1_test = []    # l2_test = []    # for i in np.linspace(0.05,1,19):    #     L1 = LR(penalty='l1',C=i,solver='liblinear',max_iter=1000)    #     L2 = LR(penalty='l2',C=i,solver='liblinear',max_iter=1000)    #     L1 = L1.fit(X_train,Y_train)    #     l1.append(accuracy_score(L1.predict(X_train),Y_train))    #     l1_test.append(accuracy_score(L1.predict(X_test),Y_test))    #    #     L2 = L2.fit(X_train,Y_train)    #     l2.append(accuracy_score(L2.predict(X_train),Y_train))    #     l2_test.append(accuracy_score(L2.predict(X_test),Y_test))    # graph = [l1,l2,l1_test,l2_test]    # color = ['green', 'black', 'lightgreen', 'gray']    # labels = ['l1', 'l2', 'l1_test', 'l2_test']    # fig = plt.figure(figsize=(10, 10))    # for i in range(len(graph)):    #     plt.plot(np.linspace(0.05,1,19),graph[i],color=color[i],label=labels[i])    # plt.legend()    # plt.show()    #0.9345777777777777    L1 = LR(penalty='l2', C=0.05, solver='saga', max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    # 模型准确率: 93.45 %    # 模型精确率: 38.60 %    # 模型召回率: 0.75 %    # 模型F1值: 1.47 %    # AUC值为： 61.02 %    # KS = 0.17    #尝试更换参数，但是并没有明显提升效果，所以更换模型def randomForestClassifier_m(X,y):    from sklearn.ensemble import RandomForestClassifier    X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)    rfc = RandomForestClassifier(n_estimators=170, max_depth=8, min_samples_leaf=4, n_jobs=-1, random_state=42,                                 class_weight="balanced")    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_predprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_predprob)    # n_estimators = 300, max_depth = 5    # 模型准确率: 93.40 %    # 模型精确率: 46.52 %    # 模型召回率: 7.97 %    # 模型F1值: 13.61 %    # AUC值为： 80.21 %    # KS = 0.47    # n_estimators = 170, max_depth = 8, min_samples_leaf = 4    # 模型准确率: 77.12 %    # 模型精确率: 18.75 %    # 模型召回率: 75.23 %    # 模型F1值: 30.02 %    # AUC值为： 83.59 %    # KS = 0.53    # scores = []    # cus_range = range(2,10,1)    # for i in cus_range:    #     print ('-'*30)    #     print (i)    #     rfc = RandomForestClassifier(n_estimators=170,max_depth=8,min_samples_leaf=4, n_jobs=-1, random_state=42, class_weight="balanced")    #     rfc = rfc.fit(X_train, Y_train)    #     y_predprob = rfc.predict_proba(X_test)    #     s = model_test_index.auc_roc_curve(Y_test, y_predprob)    #     scores.append(s)    # plt.figure(figsize=(20,5))    # plt.plot(cus_range,scores)    # plt.show()    # print ([*zip(cus_range,scores)])    # print (10+40*scores[scores.index(max(scores))],max(scores))    #第一步调试：n_estimators    #n_estimators=170    # AUC值为： 81.33 %    # KS = 0.49    #第二部调整max_depth    # max_depth=8    # AUC值为： 83.58 %    # KS = 0.53    # 第三步：min_samples_leaf    # min_samples_leaf=4    # AUC值为： 83.59 %    # KS = 0.53#主函数if __name__ == '__main__':    handle_df = get_handle_data()    X,y = scaler_data(handle_df)    randomForestClassifier_m(X,y)