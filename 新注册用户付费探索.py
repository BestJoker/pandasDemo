# coding:utf-8import pandas as pdimport numpy as npimport osimport matplotlib.pyplot as pltimport scipy.stats as stimport mathimport seaborn as snsfrom time import timeimport datetimefrom sklearn.preprocessing import OneHotEncoderimport model_test_indexfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_scorefrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.naive_bayes import GaussianNBfrom sklearn.svm import SVCfrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import KFoldfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.utils.class_weight import compute_sample_weightpd.options.mode.chained_assignment = None # 默认是'warn'plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签字体。Microsoft YaHei 或 SimHeiplt.rcParams['axes.unicode_minus']=False #用来正常显示负号def choice_model(x,y):    # prepare models    models = []    models.append(('LR', LogisticRegression()))    models.append(('LDA', LinearDiscriminantAnalysis()))    models.append(('KNN', KNeighborsClassifier()))    models.append(('CART', DecisionTreeClassifier()))    # models.append(('NB', GaussianNB()))    models.append(('SVM', SVC()))    models.append(('RFC',RandomForestClassifier()))    # evaluate each model in turn    results = []    names = []    scoring = 'accuracy'    for name, model in models:        kfold = KFold(n_splits=10, random_state=420)        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)        results.append(cv_results)        names.append(name)        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())        print(msg)    # boxplot algorithm comparison    fig = plt.figure()    fig.suptitle('Algorithm Comparison')    ax = fig.add_subplot(111)    plt.boxplot(results)    ax.set_xticklabels(names)    plt.show()#初步探索选择模型def data_handel(orign_df):    columns = [        'is_pay',        'sum_study_duration_min_3',        'registe_daus_3',        'registe_schedule_3',        'registe_shares_3',        'community_pay_rate'    ]    temp_df = orign_df[columns]    scaler = StandardScaler()  # 实例化    X = temp_df.iloc[:, temp_df.columns != 'is_pay']    y = temp_df.iloc[:, temp_df.columns == 'is_pay'].values.ravel()    trans_x = scaler.fit_transform(X)    return trans_x,y# L1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.01, max_iter=1000, class_weight='balanced')# 模型准确率: 85.87%# 模型精确率: 8.32%# 模型召回率: 71.65%# 模型F1值: 14.90%# AUC值为： 87.71%# KS = 0.62def base_LR(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.05, max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    print ('-'*30)# linear# 模型准确率: 81.24%# 模型精确率: 6.57%# 模型召回率: 76.17%# 模型F1值: 12.09%# poly# 模型准确率: 81.25%# 模型精确率: 6.57%# 模型召回率: 76.17%# 模型F1值: 12.10%# rbf# 模型准确率: 80.74%# 模型精确率: 6.77%# 模型召回率: 81.25%# 模型F1值: 12.50%# sigmoid# 模型准确率: 71.25%# 模型精确率: 4.21%# 模型召回率: 73.44%# 模型F1值: 7.96%def base_SVM(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    Kernel = ['linear', 'poly', 'rbf', 'sigmoid']    for kernel in Kernel:        print (kernel)        time0 = time()        clf = SVC(kernel=kernel                  , gamma='auto'                  , degree=1                  , cache_size=5000  # 使用多少内存来处理数据，单位MB                  ,class_weight = 'balanced'                  ).fit(X_train, Y_train)        y_predcit = clf.predict(X_test)        model_test_index.basic_data_confusion(Y_test, y_predcit)        print ('The accuracy under kernel %s is %f' % (kernel, clf.score(X_test, Y_test)))        print (datetime.datetime.fromtimestamp(time() - time0).strftime('%M:%S:%f'))#调优SVM# gamma：0.263665# 模型准确率: 81.81%# 模型精确率: 7.12%# 模型召回率: 80.86%# 模型F1值: 13.08%def iteration_SVM(x,y):    # 使用rgb调参    # 换rbf    # gamma_range = np.logspace(-5, 1, 20)    # temp_df = pd.DataFrame()    # for i in gamma_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     clf = SVC(kernel='rbf', gamma=i, cache_size=5000 ,class_weight = 'balanced').fit(X_train, Y_train)    #     y_predcit = clf.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test, y_predcit,draw_pic=False,log=False)    #     print (i,accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i':i,    #         'accuracy_score_l':accuracy_score_l,    #         'precision_score_l':precision_score_l,    #         'recall_score_l':recall_score_l,    #         'f1_score_l':f1_score_l    #     }    #     df = pd.DataFrame(dic,index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-'*30)    # print (temp_df)    # gamma=0.263665    # 0.818098    # 0.071183    # 0.808594    # 0.130847    # C_range = np.linspace(0.01,30,50)    # temp_df = pd.DataFrame()    # for i in C_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     clf = SVC(kernel='rbf', gamma=0.263665,C=i, cache_size=5000, class_weight='balanced').fit(X_train, Y_train)    #     y_predcit = clf.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,    #                                                                                                             y_predcit,    #                                                                                                             draw_pic=False,log=False)    #     print (i, accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i': i,    #         'accuracy_score_l': accuracy_score_l,    #         'precision_score_l': precision_score_l,    #         'recall_score_l': recall_score_l,    #         'f1_score_l': f1_score_l    #     }    #     df = pd.DataFrame(dic, index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-' * 30)    # print (temp_df)    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    clf = SVC(kernel='rbf', gamma=0.263665, cache_size=5000, class_weight='balanced').fit(X_train, Y_train)    y_predcit = clf.predict(X_test)    accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,y_predcit)    print (accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)# 模型准确率: 94.41%# 模型精确率: 12.86%# 模型召回率: 39.84%# 模型F1值: 19.45%def base_RFC(x,y):    rfc = RandomForestClassifier(n_estimators=50,criterion='gini',max_depth=14,random_state=420,n_jobs=-1,class_weight='balanced')    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,x,y,cv=10)    # joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc# 模型准确率: 81.51 %# 模型精确率: 7.15 %# 模型召回率: 82.81 %# 模型F1值: 13.17 %def iteration_RFC(x,y):    # estimators_range = range(1, 30, 1)    # temp_df = pd.DataFrame()    # for i in estimators_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     rfc = RandomForestClassifier(n_estimators=13, max_depth=6,min_samples_leaf=14, n_jobs=-1, random_state=420, class_weight='balanced')    #     rfc = rfc.fit(X_train, Y_train)    #     y_predcit = rfc.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,    #                                                                                                             y_predcit,    #                                                                                                             draw_pic=False,    #                                                                                                             log=False)    #     print (i, accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i': i,    #         'accuracy_score_l': accuracy_score_l,    #         'precision_score_l': precision_score_l,    #         'recall_score_l': recall_score_l,    #         'f1_score_l': f1_score_l    #     }    #     df = pd.DataFrame(dic, index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-' * 30)    # print (temp_df)    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    rfc = RandomForestClassifier(n_estimators=13, max_depth=6, min_samples_leaf=14, n_jobs=-1, random_state=420,                                 class_weight='balanced')    rfc = rfc.fit(X_train, Y_train)    y_predcit = rfc.predict(X_test)    accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,y_predcit)    print (accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    # 模型准确率: 81.51 %    # 模型精确率: 7.15 %    # 模型召回率: 82.81 %    # 模型F1值: 13.17 %#主函数if __name__ == '__main__':    # file_1_name = '2021-02-25 注册用户高潜数据(2021-01-26~2021-02-25)-V2.xlsx'    # file_2_name = '2021-01-15 注册用户高潜数据(2020-12-16~2021-01-15)-V2.xlsx'    # PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    # path_1 = os.path.join(PROJECT_ROOT,'data/'+file_1_name)    # orign_1_df = pd.read_excel(path_1)    # print(orign_1_df.shape)    #    # path_2 = os.path.join(PROJECT_ROOT, 'data/' + file_2_name)    # orign_2_df = pd.read_excel(path_2)    # print(orign_2_df.shape)    PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    # file_name = '2020-12-16~2021-02-25新注册用户数据.xlsx'    file_name = '2021-02-25 注册用户高潜数据(2021-01-26~2021-02-25)-V2.xlsx'    # 准确度accuracy「(TP + TN) / (TP + TN + FN + TN)」： 0.5084705269102124    # 精度precision「TP / (TP + FP)」： 0.02917552137737276    # 召回recall「TP / (TP + FP)」： 0.9642857142857143    # F1值「2 * TP / (2 * TP + FP + FN)」： 0.05663741565570045    path = os.path.join(PROJECT_ROOT, 'data/' + file_name)    orign_df = pd.read_excel(path, index=False)    str_columns = orign_df.select_dtypes(include=['object', 'datetime']).columns    num_columns = orign_df.select_dtypes(include=['number']).columns    orign_df[str_columns] = orign_df[str_columns].fillna('')    orign_df[num_columns] = orign_df[num_columns].fillna(value=0)    #去除团报，保留正常用户    orign_df = orign_df[(orign_df['is_group_pay']==0)]    orign_df.info()    X,y = data_handel(orign_df)    iteration_RFC(X,y)    # iteration_SVM(X,y)    # choice_model(X,y)