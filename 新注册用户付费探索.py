# coding:utf-8import pandas as pdimport numpy as npimport osimport matplotlib.pyplot as pltimport scipy.stats as stimport mathimport seaborn as snsfrom time import timeimport datetimefrom sklearn.preprocessing import OneHotEncoderimport model_test_indexfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_scorefrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.naive_bayes import GaussianNBfrom sklearn.svm import SVCfrom sklearn.linear_model import LogisticRegressionfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.model_selection import KFoldfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.utils.class_weight import compute_sample_weightpd.options.mode.chained_assignment = None # 默认是'warn'plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签字体。Microsoft YaHei 或 SimHeiplt.rcParams['axes.unicode_minus']=False #用来正常显示负号#初步筛选模型def choice_model(x,y):    # prepare models    models = []    models.append(('LR', LogisticRegression()))    models.append(('LDA', LinearDiscriminantAnalysis()))    models.append(('KNN', KNeighborsClassifier()))    models.append(('CART', DecisionTreeClassifier()))    # models.append(('NB', GaussianNB()))    models.append(('SVM', SVC()))    models.append(('RFC',RandomForestClassifier()))    # evaluate each model in turn    results = []    names = []    scoring = 'accuracy'    for name, model in models:        kfold = KFold(n_splits=10, random_state=420)        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)        results.append(cv_results)        names.append(name)        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())        print(msg)    # boxplot algorithm comparison    fig = plt.figure()    fig.suptitle('Algorithm Comparison')    ax = fig.add_subplot(111)    plt.boxplot(results)    ax.set_xticklabels(names)    plt.show()#初步探索选择模型def data_handel(orign_df):    columns = [        'is_pay',        'sum_study_duration_min_3',        'registe_daus_3',        'registe_schedule_3',        'registe_shares_3',        'community_pay_rate'    ]    temp_df = orign_df[columns]    scaler = StandardScaler()  # 实例化    X = temp_df.iloc[:, temp_df.columns != 'is_pay']    y = temp_df.iloc[:, temp_df.columns == 'is_pay'].values.ravel()    trans_x = scaler.fit_transform(X)    return trans_x,y# L1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.01, max_iter=1000, class_weight='balanced')# 模型准确率: 85.99%# 模型精确率: 8.39%# 模型召回率: 75.25%# 模型F1值: 15.10%# AUC值为： 88.80%# KS = 0.64def base_LR(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    L1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.05, max_iter=1000, class_weight='balanced')    L1 = L1.fit(X_train, Y_train)    y_predcit = L1.predict(X_test)    model_test_index.basic_data_confusion(Y_test, y_predcit)    y_predprob = L1.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test, y_predprob)    return L1    print ('-'*30)# linear# 模型准确率: 81.24%# 模型精确率: 6.57%# 模型召回率: 76.17%# 模型F1值: 12.09%# poly# 模型准确率: 81.25%# 模型精确率: 6.57%# 模型召回率: 76.17%# 模型F1值: 12.10%# rbf# 模型准确率: 80.74%# 模型精确率: 6.77%# 模型召回率: 81.25%# 模型F1值: 12.50%# sigmoid# 模型准确率: 71.25%# 模型精确率: 4.21%# 模型召回率: 73.44%# 模型F1值: 7.96%def base_SVM(x,y):    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    Kernel = ['linear', 'poly', 'rbf', 'sigmoid']    for kernel in Kernel:        print (kernel)        time0 = time()        clf = SVC(kernel=kernel                  , gamma='auto'                  , degree=1                  , cache_size=5000  # 使用多少内存来处理数据，单位MB                  ,class_weight = 'balanced'                  ).fit(X_train, Y_train)        y_predcit = clf.predict(X_test)        model_test_index.basic_data_confusion(Y_test, y_predcit)        print ('The accuracy under kernel %s is %f' % (kernel, clf.score(X_test, Y_test)))        print (datetime.datetime.fromtimestamp(time() - time0).strftime('%M:%S:%f'))#调优SVM# gamma：0.263665# 模型准确率: 82.25%# 模型精确率: 7.36%# 模型召回率: 83.83%# 模型F1值: 13.53%def iteration_SVM(x,y):    # 使用rgb调参    # 换rbf    # gamma_range = np.logspace(-5, 1, 20)    # temp_df = pd.DataFrame()    # for i in gamma_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     clf = SVC(kernel='rbf', gamma=i, cache_size=5000 ,class_weight = 'balanced').fit(X_train, Y_train)    #     y_predcit = clf.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test, y_predcit,draw_pic=False,log=False)    #     print (i,accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i':i,    #         'accuracy_score_l':accuracy_score_l,    #         'precision_score_l':precision_score_l,    #         'recall_score_l':recall_score_l,    #         'f1_score_l':f1_score_l    #     }    #     df = pd.DataFrame(dic,index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-'*30)    # print (temp_df)    # gamma=0.263665    # 0.818098    # 0.071183    # 0.808594    # 0.130847    # C_range = np.linspace(0.01,30,50)    # temp_df = pd.DataFrame()    # for i in C_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     clf = SVC(kernel='rbf', gamma=0.263665,C=i, cache_size=5000, class_weight='balanced').fit(X_train, Y_train)    #     y_predcit = clf.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,    #                                                                                                             y_predcit,    #                                                                                                             draw_pic=False,log=False)    #     print (i, accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i': i,    #         'accuracy_score_l': accuracy_score_l,    #         'precision_score_l': precision_score_l,    #         'recall_score_l': recall_score_l,    #         'f1_score_l': f1_score_l    #     }    #     df = pd.DataFrame(dic, index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-' * 30)    # print (temp_df)    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    clf = SVC(kernel='rbf', gamma=0.263665, cache_size=5000, class_weight='balanced').fit(X_train, Y_train)    y_predcit = clf.predict(X_test)    accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,y_predcit)    print (accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    return clf# 模型准确率: 94.41%# 模型精确率: 12.86%# 模型召回率: 39.84%# 模型F1值: 19.45%def base_RFC(x,y):    rfc = RandomForestClassifier(n_estimators=50,criterion='gini',max_depth=14,random_state=420,n_jobs=-1,class_weight='balanced')    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    rfc = rfc.fit(X_train,Y_train)    y_predict = rfc.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = rfc.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    model_test_index.corss_val_score_cus(rfc,x,y,cv=10)    # joblib.dump(rfc,get_file_path('rfc.pkl'))    return rfc# 模型准确率: 82.45%# 模型精确率: 7.48%# 模型召回率: 84.49%# 模型F1值: 13.75%def iteration_RFC(x,y):    # estimators_range = np.arange(36,42,0.5)    # temp_df = pd.DataFrame()    # for i in estimators_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     rfc = RandomForestClassifier(n_estimators=13, max_depth=6,min_samples_leaf=14, n_jobs=-1, random_state=420)    #     class_weight = {0:1,1:i}    #     sw = compute_sample_weight(class_weight=class_weight, y=Y_train)    #     rfc = rfc.fit(X_train, Y_train,sample_weight=sw)    #     y_predcit = rfc.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,    #                                                                                                             y_predcit,    #                                                                                                             draw_pic=False,    #                                                                                                             log=False)    #     print (i, accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i': i,    #         'accuracy_score_l': accuracy_score_l,    #         'precision_score_l': precision_score_l,    #         'recall_score_l': recall_score_l,    #         'f1_score_l': f1_score_l    #     }    #     df = pd.DataFrame(dic, index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-' * 30)    # print (temp_df)    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    rfc = RandomForestClassifier(n_estimators=13, max_depth=6, min_samples_leaf=14, n_jobs=-1, random_state=420,                                 class_weight='balanced')    rfc = rfc.fit(X_train, Y_train)    y_predcit = rfc.predict(X_test)    accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,y_predcit)    print (accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    return rfc# 模型准确率: 85.66%# 模型精确率: 8.75%# 模型召回率: 81.19%# 模型F1值: 15.79%def iteration_GTDB(x,y):    # estimators_range = np.arange(45,55,0.5)    # temp_df = pd.DataFrame()    # for i in estimators_range:    #     X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=420)    #     rfc = GradientBoostingClassifier(n_estimators=58,subsample=0.531053,max_depth=3,random_state=420)    #     class_weight = {0:1,1:i}    #     sw = compute_sample_weight(class_weight=class_weight, y=Y_train)    #     rfc = rfc.fit(X_train, Y_train,sample_weight=sw)    #     y_predcit = rfc.predict(X_test)    #     accuracy_score_l, precision_score_l, recall_score_l, f1_score_l = model_test_index.basic_data_confusion(Y_test,    #                                                                                                             y_predcit,    #                                                                                                             draw_pic=False,    #                                                                                                             log=False)    #     print (i, accuracy_score_l, precision_score_l, recall_score_l, f1_score_l)    #     dic = {    #         'i': i,    #         'accuracy_score_l': accuracy_score_l,    #         'precision_score_l': precision_score_l,    #         'recall_score_l': recall_score_l,    #         'f1_score_l': f1_score_l    #     }    #     df = pd.DataFrame(dic, index=[0])    #     print (df)    #     temp_df = temp_df.append(df)    #     print (temp_df.shape)    #     print('-' * 30)    # print (temp_df)    X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3,random_state=420)    gbm0 = GradientBoostingClassifier(n_estimators=58, subsample=0.531053, max_depth=3, random_state=420)    weight = {0:1,1:47.5}    sw = compute_sample_weight(class_weight=weight, y=Y_train)    gbm0 = gbm0.fit(X_train, Y_train, sample_weight=sw)    y_predict = gbm0.predict(X_test)    model_test_index.basic_data_confusion(Y_test,y_predict)    y_preprob = gbm0.predict_proba(X_test)    model_test_index.auc_roc_curve(Y_test,y_preprob)    return gbm0#训练和预测def balance_train_predcit_model(train_X,train_Y,test_X,test_Y):    # 2月份数据预测值    # 模型准确率: 84.85 %    # 模型精确率: 7.55 %    # 模型召回率: 72.29 %    # 模型F1值: 13.68 %    # model = base_LR(train_X,train_Y)    # 模型准确率: 77.02 %    # 模型精确率: 5.87 %    # 模型召回率: 85.37 %    # 模型F1值: 10.98 %    # model = iteration_SVM(train_X,train_Y)    print ('训练集'+'-' * 30)    # 模型准确率: 76.43 %    # 模型精确率: 5.80 %    # 模型召回率: 86.56 %    # 模型F1值: 10.87 %    model = iteration_RFC(train_X,train_Y)    # 模型准确率: 81.74 %    # 模型精确率: 7.00 %    # 模型召回率: 81.33 %    # 模型F1值: 12.88 %    # model = iteration_GTDB(train_X,train_Y)    #预测真实值    # print ('真实值预测'+'*' * 30)    # y_predcit = model.predict(test_X)    # model_test_index.basic_data_confusion(test_Y, y_predcit)    #    # y_predprob = model.predict_proba(test_X)    # model_test_index.auc_roc_curve(test_Y, y_predprob)def get_data(file_name):    PROJECT_ROOT = os.path.dirname(os.path.realpath(__file__))    path = os.path.join(PROJECT_ROOT, 'data/' + file_name)    orign_df = pd.read_excel(path, index=False)    str_columns = orign_df.select_dtypes(include=['object', 'datetime']).columns    num_columns = orign_df.select_dtypes(include=['number']).columns    orign_df[str_columns] = orign_df[str_columns].fillna('')    orign_df[num_columns] = orign_df[num_columns].fillna(value=0)    # 去除团报，保留正常用户    orign_df = orign_df[(orign_df['is_group_pay'] == 0)]    print (orign_df.shape)    X, y = data_handel(orign_df)    return X,y#主函数if __name__ == '__main__':    #上一个月的数据    train_file_name = '2021-01-31 注册用户高潜数据(2021-01-01~2021-01-31)-V2.xlsx'    train_X,train_Y = get_data(train_file_name)    #最近30天的数据    # test_file_name = '2021-03-01 注册用户高潜数据(2021-01-30~2021-03-01)-V2.xlsx'    # test_X,test_Y = get_data(test_file_name)    test_X, test_Y = 0,0    balance_train_predcit_model(train_X,train_Y,test_X,test_Y)